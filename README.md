# Vocab Analyzer - è‹±æ–‡ä¹¦è¯æ±‡ç­‰çº§åˆ†æå·¥å…·

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

<a name="english"></a>
## English

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Production Ready](https://img.shields.io/badge/status-production%20ready-brightgreen.svg)](https://github.com/yourusername/vocab-analyzer)

A powerful CLI tool that analyzes English books and generates vocabulary lists organized by CEFR levels (A1-C2+) with Chinese translations, phrasal verb detection, and comprehensive statistics.

### ğŸ¯ Key Features

- **Interactive Web Interface**: User-friendly web UI with real-time progress, filtering, and search
- **Multi-format Support**: Extract and analyze vocabulary from TXT, PDF, DOCX, and JSON files
- **CEFR Classification**: Classify words by 7 CEFR levels (A1, A2, B1, B2, C1, C2, C2+)
- **Phrasal Verb Detection**: Recognize 124+ common phrasal verbs, including separable forms
- **Chinese Translations**: Integrate 770K+ word ECDICT dictionary with Chinese definitions
- **Multiple Export Formats**: Export results in JSON, CSV, and Markdown formats
- **Statistical Analysis**: Comprehensive vocabulary distribution and insights
- **High Performance**: Process 100-page books in under 60 seconds
- **Beautiful CLI**: Rich terminal interface with progress bars and colored output

### ğŸŒ Bilingual UI & Translation

New translation capabilities have been added:
- **Bilingual Interface**: Simultaneous English/Chinese UI display
- **Offline Translation**: Three-tier translation (ECDICT â†’ Mdict â†’ Argos Translate)
- **CEFR Education**: Interactive level descriptions and learning guidance
- **Translation Caching**: Persistent cache for improved performance
- **Configurable**: YAML-based configuration for all translation settings

### ğŸ“¦ Installation

#### Prerequisites

- Python 3.10 or higher
- pip package manager

#### Quick Start

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/vocab-analyzer.git
cd vocab-analyzer
```

2. **Create a virtual environment** (recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install the package**:
```bash
pip install -e .
```

4. **Download the spaCy language model**:
```bash
python -m spacy download en_core_web_sm
```

#### Development Installation

For development with additional tools:
```bash
pip install -e ".[dev]"
pre-commit install
```

### ğŸš€ Usage

#### Basic Commands

**Analyze a book** (generates all formats):
```bash
vocab-analyzer analyze book.txt
```

**View statistics only**:
```bash
vocab-analyzer stats book.txt
```

**Extract specific CEFR levels**:
```bash
vocab-analyzer extract book.txt --levels B2 C1
```

#### Advanced Usage

**Specify output format**:
```bash
# Single format
vocab-analyzer analyze book.txt --format json
vocab-analyzer analyze book.txt --format csv
vocab-analyzer analyze book.txt --format markdown

# Custom output path
vocab-analyzer analyze book.pdf --output results/my_vocab.json
```

**Filter by CEFR level**:
```bash
# Only B2-C1 words
vocab-analyzer analyze book.txt --min-level B2 --max-level C1

# Advanced words only (C1+)
vocab-analyzer analyze book.txt --min-level C1
```

**Batch processing**:
```bash
# Analyze multiple books
for book in books/*.txt; do
    vocab-analyzer analyze "$book" --output "results/$(basename "$book" .txt)_vocab.json"
done
```

#### Example Output

**Terminal Statistics**:
```
ğŸ“š Analyzing: pride_and_prejudice.txt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%

ğŸ“Š Vocabulary Analysis Complete
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total words: 127,377
Unique words: 6,544
Unique phrases: 18

Level Distribution:
  A1 (KET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  569 (8.7%)
  A2 (PET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  906 (13.8%)
  B1 (FCE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  2,145 (32.8%)
  B2 (CAE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  1,456 (22.3%)
  C1        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  892 (13.6%)
  C2        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  398 (6.1%)
  C2+ (è¶…çº²) â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  178 (2.7%)

âœ… Output files generated:
  â€¢ pride_and_prejudice_vocabulary.json
  â€¢ pride_and_prejudice_vocabulary.csv
  â€¢ pride_and_prejudice_vocabulary_words.csv
  â€¢ pride_and_prejudice_vocabulary_phrases.csv
  â€¢ pride_and_prejudice_vocabulary.md

ğŸ’¡ Recommended for IELTS 6.5-7.5 level readers
```

#### ğŸŒ Web Interface

Launch the interactive web interface for a more user-friendly experience:

```bash
vocab-analyzer web
```

The web interface provides:
- **Drag-and-drop file upload** (TXT, PDF, DOCX)
- **Real-time progress tracking** with Server-Sent Events
- **Interactive results visualization**
  - Filter words by CEFR level (A1-C2+)
  - Search for specific words in real-time
  - Click words to see detailed information with Chinese translations
- **Visual statistics** with CEFR distribution charts
- **Multiple download formats** (JSON, CSV, Markdown)

**Access the interface at**: `http://127.0.0.1:5000`

**Custom port and debug mode**:
```bash
vocab-analyzer web --port 8080 --debug
```

**Browser requirements**: Modern browsers (Chrome, Firefox, Safari, Edge) with JavaScript enabled

### ğŸ—ï¸ Architecture

#### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VocabularyAnalyzer                        â”‚
â”‚                    (Facade Pattern)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
    â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Config  â”‚      â”‚ Components  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Extractorâ”‚    â”‚Processor â”‚   â”‚ Matcher  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Analyzer â”‚    â”‚Exporter  â”‚   â”‚   CLI    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Design Patterns

1. **Facade Pattern**: `VocabularyAnalyzer` provides a simplified interface to complex subsystems
2. **Strategy Pattern**: Multiple extractors (TXT, PDF, DOCX) and exporters (JSON, CSV, MD)
3. **Singleton Pattern**: Shared spaCy model instance for performance
4. **Factory Pattern**: Dynamic extractor selection based on file type
5. **Template Method**: `BaseExtractor` abstract class for common extraction logic
6. **Dependency Injection**: Configuration-driven component initialization

#### Core Components

**1. Text Extractors** (`src/vocab_analyzer/extractors/`)
- `TxtExtractor`: Plain text files (UTF-8)
- `PdfExtractor`: PDF documents (PyPDF2)
- `DocxExtractor`: Word documents (python-docx)
- `JsonExtractor`: Structured JSON data

**2. NLP Processors** (`src/vocab_analyzer/processors/`)
- `Tokenizer`: spaCy-based tokenization, lemmatization, POS tagging
- `PhraseDetector`: Dependency parsing for phrasal verb detection

**3. Level Matcher** (`src/vocab_analyzer/matchers/`)
- CEFR level classification (43,699 classified words)
- Oxford 3000 marker integration
- Frequency-based level inference
- LRU cache for fast lookups (10,000 entries)

**4. Analyzers** (`src/vocab_analyzer/analyzers/`)
- `StatisticsAnalyzer`: Comprehensive vocabulary statistics
- Level distribution calculation
- Frequency analysis

**5. Exporters** (`src/vocab_analyzer/exporters/`)
- `JsonExporter`: Structured JSON with metadata
- `CsvExporter`: Excel-compatible CSV (separate word and phrase files)
- `MarkdownExporter`: Human-readable Markdown reports

#### Data Flow

```
Input File
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Extraction â”‚ â†’ Raw text
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NLP Processing  â”‚ â†’ Tokens (lemmatized, POS-tagged)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phrase Detectionâ”‚ â†’ Phrasal verbs identified
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Level Matching  â”‚ â†’ CEFR levels assigned
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Statistics      â”‚ â†’ Analysis results
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Export          â”‚ â†’ JSON/CSV/Markdown files
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‚ Project Structure

```
vocab-analyzer/
â”œâ”€â”€ src/vocab_analyzer/          # Source code
â”‚   â”œâ”€â”€ models/                  # Data models
â”‚   â”œâ”€â”€ extractors/              # Text extraction
â”‚   â”œâ”€â”€ processors/              # NLP processing
â”‚   â”œâ”€â”€ matchers/                # Level matching
â”‚   â”œâ”€â”€ analyzers/               # Statistics
â”‚   â”œâ”€â”€ exporters/               # Output formats
â”‚   â”œâ”€â”€ core/                    # Core logic
â”‚   â”œâ”€â”€ cli/                     # CLI interface
â”‚   â”œâ”€â”€ web/                     # Web interface
â”‚   â”œâ”€â”€ translation/             # Translation components
â”‚   â””â”€â”€ utils/                   # Utilities
â”œâ”€â”€ data/                        # Data resources
â”‚   â”œâ”€â”€ vocabularies/            # CEFR wordlists
â”‚   â”œâ”€â”€ phrases/                 # Phrasal verbs
â”‚   â”œâ”€â”€ dictionaries/            # Dictionaries
â”‚   â”œâ”€â”€ sample_books/            # Sample texts
â”‚   â””â”€â”€ mappings/                # CEFR-IELTS mapping
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ conftest.py             # Test fixtures
â”‚   â”œâ”€â”€ unit/                   # Unit tests
â”‚   â””â”€â”€ integration/            # Integration tests
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ prepare_data.py         # Data preparation
â”‚   â””â”€â”€ validate_data.py        # Data validation
â”œâ”€â”€ config/                      # Configuration
â”‚   â””â”€â”€ default_config.yaml     # Default settings
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ USER_GUIDE.md           # User guide
â”‚   â”œâ”€â”€ EXAMPLES.md             # Examples
â”‚   â””â”€â”€ QUICK_REFERENCE.md      # Quick reference
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ setup.py                     # Package setup
â””â”€â”€ README.md                    # This file
```

### ğŸ“Š Performance

**Benchmarks** (tested on MacBook Pro M1):
- Small files (<5 pages): ~2 seconds
- Medium files (20-50 pages): ~15 seconds
- Large files (100+ pages): <60 seconds
- Memory usage: <400MB peak

**Optimizations**:
- Global spaCy model loading (Singleton pattern)
- LRU cache for word/phrase lookups (10,000 entries)
- Batch processing (100 sentences per batch)
- pandas DataFrame indexing

### ğŸ—‚ï¸ Data Sources

This project uses the following open-source data:

| Resource | Size | License | Source |
|----------|------|---------|--------|
| **ECDICT** | 770,608 words | MIT | [skywind3000/ECDICT](https://github.com/skywind3000/ECDICT) |
| **Phrasal Verbs** | 124 verbs | Open Source | [Semigradsky/phrasal-verbs](https://github.com/Semigradsky/phrasal-verbs) |
| **Sample Books** | 3 books | Public Domain | [Project Gutenberg](https://www.gutenberg.org/) |

### ğŸ§ª Development

#### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=vocab_analyzer --cov-report=html

# Run specific test
pytest tests/unit/test_word.py -v

# Run with verbose output
pytest -vv
```

#### Code Quality

```bash
# Format code
black src/ tests/
isort src/ tests/

# Check code style
flake8 src/ tests/
pylint src/ --rcfile=.pylintrc

# Type checking
mypy src/
```

#### Pre-commit Hooks

```bash
# Install hooks (runs on every commit)
pre-commit install

# Run manually on all files
pre-commit run --all-files
```

### âš™ï¸ Configuration

Default configuration is in `config/default_config.yaml`. Override with custom config:

```bash
vocab-analyzer analyze book.txt --config my_config.yaml
```

**Key configuration options**:
- NLP model and batch size
- Data file paths (dictionaries, wordlists)
- Output formats and templates
- Analysis parameters (min frequency, level thresholds)
- Logging level and format

### ğŸ—ºï¸ Roadmap

#### âœ… Phase 1: MVP (v0.1.0 - Completed)
- [x] Multi-format text extraction (TXT, PDF, DOCX, JSON)
- [x] CEFR level classification (A1-C2+)
- [x] Phrasal verb detection (124 verbs)
- [x] Chinese translations (770K+ words)
- [x] Multiple export formats (JSON, CSV, Markdown)
- [x] CLI with rich formatting
- [x] Comprehensive statistics
- [x] Web interface (Flask/FastAPI)

#### ğŸ”„ Phase 2: Enhancements (Planned)
- [ ] Expand phrasal verb dictionary (500+ verbs)
- [ ] Advanced example sentence extraction
- [ ] Anki deck export format
- [ ] Batch processing mode

#### ğŸš€ Phase 3: Advanced Features (Future)
- [ ] Progress tracking and learning analytics
- [ ] Custom wordlist support
- [ ] Audio pronunciation integration
- [ ] Multi-language support (French, German, Spanish)
- [ ] Mobile application

### ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ™ Acknowledgments

- **ECDICT** for the comprehensive English-Chinese dictionary
- **spaCy** for powerful NLP processing
- **Project Gutenberg** for public domain books
- All open-source contributors

### ğŸ“ Support

For issues, questions, or suggestions:
- ğŸ“– Check [USER_GUIDE.md](docs/USER_GUIDE.md) for detailed usage
- ğŸ“ Review [EXAMPLES.md](docs/EXAMPLES.md) for practical examples
- ğŸ› Open an issue on GitHub
- ğŸ’¬ Join discussions in GitHub Discussions

---

<a name="ä¸­æ–‡"></a>
## ä¸­æ–‡

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: ç”Ÿäº§å°±ç»ª](https://img.shields.io/badge/status-%E7%94%9F%E4%BA%A7%E5%B0%B1%E7%BB%AA-brightgreen.svg)](https://github.com/yourusername/vocab-analyzer)

ä¸€ä¸ªå¼ºå¤§çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå¯ä»¥åˆ†æè‹±æ–‡ä¹¦ç±å¹¶ç”ŸæˆæŒ‰CEFRç­‰çº§ï¼ˆA1-C2+ï¼‰åˆ†ç±»çš„è¯æ±‡è¡¨ï¼ŒåŒ…å«ä¸­æ–‡ç¿»è¯‘ã€è¯ç»„è¯†åˆ«å’Œå…¨é¢çš„ç»Ÿè®¡åˆ†æã€‚

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

- **å¤šæ ¼å¼æ”¯æŒ**ï¼šä»TXTã€PDFã€DOCXã€JSONæ–‡ä»¶ä¸­æå–å’Œåˆ†æè¯æ±‡
- **CEFRåˆ†çº§**ï¼šå°†å•è¯åˆ†ä¸º7ä¸ªCEFRç­‰çº§ï¼ˆA1, A2, B1, B2, C1, C2, C2+ï¼‰
- **è¯ç»„è¯†åˆ«**ï¼šè¯†åˆ«124+ä¸ªå¸¸ç”¨åŠ¨è¯çŸ­è¯­ï¼ŒåŒ…æ‹¬å¯åˆ†ç¦»å½¢å¼
- **ä¸­æ–‡ç¿»è¯‘**ï¼šé›†æˆ770K+è¯æ¡çš„ECDICTè¯å…¸ï¼Œæä¾›ä¸­æ–‡é‡Šä¹‰
- **å¤šç§å¯¼å‡ºæ ¼å¼**ï¼šæ”¯æŒJSONã€CSVã€Markdownæ ¼å¼å¯¼å‡º
- **ç»Ÿè®¡åˆ†æ**ï¼šå…¨é¢çš„è¯æ±‡åˆ†å¸ƒå’Œæ´å¯Ÿ
- **é«˜æ€§èƒ½**ï¼š100é¡µä¹¦ç±å¤„ç†æ—¶é—´å°äº60ç§’
- **ç²¾ç¾CLI**ï¼šä¸°å¯Œçš„ç»ˆç«¯ç•Œé¢ï¼Œå¸¦è¿›åº¦æ¡å’Œå½©è‰²è¾“å‡º

### ğŸ“¦ å®‰è£…

#### ç³»ç»Ÿè¦æ±‚

- Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- pip åŒ…ç®¡ç†å™¨

#### å¿«é€Ÿå¼€å§‹

1. **å…‹éš†ä»“åº“**ï¼š
```bash
git clone https://github.com/yourusername/vocab-analyzer.git
cd vocab-analyzer
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**ï¼ˆæ¨èï¼‰ï¼š
```bash
python -m venv venv
source venv/bin/activate  # Windowsç³»ç»Ÿ: venv\Scripts\activate
```

3. **å®‰è£…åŒ…**ï¼š
```bash
pip install -e .
```

4. **ä¸‹è½½spaCyè¯­è¨€æ¨¡å‹**ï¼š
```bash
python -m spacy download en_core_web_sm
```

#### å¼€å‘ç¯å¢ƒå®‰è£…

åŒ…å«é¢å¤–å¼€å‘å·¥å…·çš„å®‰è£…ï¼š
```bash
pip install -e ".[dev]"
pre-commit install
```

### ğŸš€ ä½¿ç”¨æ–¹æ³•

#### åŸºæœ¬å‘½ä»¤

**åˆ†æä¸€æœ¬ä¹¦**ï¼ˆç”Ÿæˆæ‰€æœ‰æ ¼å¼ï¼‰ï¼š
```bash
vocab-analyzer analyze book.txt
```

**ä»…æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯**ï¼š
```bash
vocab-analyzer stats book.txt
```

**æå–ç‰¹å®šCEFRç­‰çº§**ï¼š
```bash
vocab-analyzer extract book.txt --levels B2 C1
```

#### é«˜çº§ç”¨æ³•

**æŒ‡å®šè¾“å‡ºæ ¼å¼**ï¼š
```bash
# å•ä¸€æ ¼å¼
vocab-analyzer analyze book.txt --format json
vocab-analyzer analyze book.txt --format csv
vocab-analyzer analyze book.txt --format markdown

# è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„
vocab-analyzer analyze book.pdf --output results/my_vocab.json
```

**æŒ‰CEFRç­‰çº§è¿‡æ»¤**ï¼š
```bash
# ä»…B2-C1å•è¯
vocab-analyzer analyze book.txt --min-level B2 --max-level C1

# ä»…é«˜çº§è¯æ±‡ï¼ˆC1+ï¼‰
vocab-analyzer analyze book.txt --min-level C1
```

**æ‰¹é‡å¤„ç†**ï¼š
```bash
# åˆ†æå¤šæœ¬ä¹¦
for book in books/*.txt; do
    vocab-analyzer analyze "$book" --output "results/$(basename "$book" .txt)_vocab.json"
done
```

#### ç¤ºä¾‹è¾“å‡º

**ç»ˆç«¯ç»Ÿè®¡**ï¼š
```
ğŸ“š æ­£åœ¨åˆ†æ: å‚²æ…¢ä¸åè§.txt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%

ğŸ“Š è¯æ±‡åˆ†æå®Œæˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»è¯æ•°: 127,377
ç‹¬ç«‹å•è¯: 6,544
è¯ç»„æ•°é‡: 18

ç­‰çº§åˆ†å¸ƒ:
  A1 (KET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  569 (8.7%)
  A2 (PET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  906 (13.8%)
  B1 (FCE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  2,145 (32.8%)
  B2 (CAE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  1,456 (22.3%)
  C1        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  892 (13.6%)
  C2        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  398 (6.1%)
  C2+ (è¶…çº²) â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  178 (2.7%)

âœ… å·²ç”Ÿæˆè¾“å‡ºæ–‡ä»¶:
  â€¢ pride_and_prejudice_vocabulary.json
  â€¢ pride_and_prejudice_vocabulary.csv
  â€¢ pride_and_prejudice_vocabulary_words.csv
  â€¢ pride_and_prejudice_vocabulary_phrases.csv
  â€¢ pride_and_prejudice_vocabulary.md

ğŸ’¡ æ¨èç»™é›…æ€6.5-7.5åˆ†æ°´å¹³çš„è¯»è€…
```

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

#### æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VocabularyAnalyzer                        â”‚
â”‚                    (å¤–è§‚æ¨¡å¼)                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
    â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®    â”‚      â”‚ æ ¸å¿ƒç»„ä»¶    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚æ–‡æœ¬æå– â”‚    â”‚NLPå¤„ç†   â”‚   â”‚ç­‰çº§åŒ¹é…  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ç»Ÿè®¡åˆ†æ â”‚    â”‚æ ¼å¼å¯¼å‡º  â”‚   â”‚ CLIç•Œé¢  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### è®¾è®¡æ¨¡å¼

1. **å¤–è§‚æ¨¡å¼**ï¼š`VocabularyAnalyzer`ä¸ºå¤æ‚å­ç³»ç»Ÿæä¾›ç®€åŒ–æ¥å£
2. **ç­–ç•¥æ¨¡å¼**ï¼šå¤šç§æå–å™¨ï¼ˆTXTã€PDFã€DOCXï¼‰å’Œå¯¼å‡ºå™¨ï¼ˆJSONã€CSVã€MDï¼‰
3. **å•ä¾‹æ¨¡å¼**ï¼šå…±äº«spaCyæ¨¡å‹å®ä¾‹ä»¥æå‡æ€§èƒ½
4. **å·¥å‚æ¨¡å¼**ï¼šæ ¹æ®æ–‡ä»¶ç±»å‹åŠ¨æ€é€‰æ‹©æå–å™¨
5. **æ¨¡æ¿æ–¹æ³•**ï¼š`BaseExtractor`æŠ½è±¡ç±»å®šä¹‰é€šç”¨æå–é€»è¾‘
6. **ä¾èµ–æ³¨å…¥**ï¼šé…ç½®é©±åŠ¨çš„ç»„ä»¶åˆå§‹åŒ–

#### æ ¸å¿ƒç»„ä»¶

**1. æ–‡æœ¬æå–å™¨**ï¼ˆ`src/vocab_analyzer/extractors/`ï¼‰
- `TxtExtractor`ï¼šçº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆUTF-8ï¼‰
- `PdfExtractor`ï¼šPDFæ–‡æ¡£ï¼ˆPyPDF2ï¼‰
- `DocxExtractor`ï¼šWordæ–‡æ¡£ï¼ˆpython-docxï¼‰
- `JsonExtractor`ï¼šç»“æ„åŒ–JSONæ•°æ®

**2. NLPå¤„ç†å™¨**ï¼ˆ`src/vocab_analyzer/processors/`ï¼‰
- `Tokenizer`ï¼šåŸºäºspaCyçš„åˆ†è¯ã€è¯å½¢è¿˜åŸã€è¯æ€§æ ‡æ³¨
- `PhraseDetector`ï¼šä¾å­˜å¥æ³•åˆ†æè¿›è¡ŒåŠ¨è¯çŸ­è¯­æ£€æµ‹

**3. ç­‰çº§åŒ¹é…å™¨**ï¼ˆ`src/vocab_analyzer/matchers/`ï¼‰
- CEFRç­‰çº§åˆ†ç±»ï¼ˆ43,699ä¸ªå·²åˆ†ç±»å•è¯ï¼‰
- Oxford 3000æ ‡è®°é›†æˆ
- åŸºäºé¢‘ç‡çš„ç­‰çº§æ¨æ–­
- LRUç¼“å­˜å¿«é€ŸæŸ¥è¯¢ï¼ˆ10,000æ¡ï¼‰

**4. ç»Ÿè®¡åˆ†æå™¨**ï¼ˆ`src/vocab_analyzer/analyzers/`ï¼‰
- `StatisticsAnalyzer`ï¼šå…¨é¢çš„è¯æ±‡ç»Ÿè®¡
- ç­‰çº§åˆ†å¸ƒè®¡ç®—
- é¢‘ç‡åˆ†æ

**5. æ ¼å¼å¯¼å‡ºå™¨**ï¼ˆ`src/vocab_analyzer/exporters/`ï¼‰
- `JsonExporter`ï¼šå¸¦å…ƒæ•°æ®çš„ç»“æ„åŒ–JSON
- `CsvExporter`ï¼šExcelå…¼å®¹çš„CSVï¼ˆå•è¯å’Œè¯ç»„åˆ†ç¦»æ–‡ä»¶ï¼‰
- `MarkdownExporter`ï¼šäººç±»å¯è¯»çš„MarkdownæŠ¥å‘Š

#### æ•°æ®æµç¨‹

```
è¾“å…¥æ–‡ä»¶
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–‡æœ¬æå–        â”‚ â†’ åŸå§‹æ–‡æœ¬
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NLPå¤„ç†         â”‚ â†’ è¯å…ƒï¼ˆè¯å½¢è¿˜åŸã€è¯æ€§æ ‡æ³¨ï¼‰
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¯ç»„æ£€æµ‹        â”‚ â†’ è¯†åˆ«åŠ¨è¯çŸ­è¯­
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç­‰çº§åŒ¹é…        â”‚ â†’ åˆ†é…CEFRç­‰çº§
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç»Ÿè®¡åˆ†æ        â”‚ â†’ åˆ†æç»“æœ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ ¼å¼å¯¼å‡º        â”‚ â†’ JSON/CSV/Markdownæ–‡ä»¶
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‚ é¡¹ç›®ç»“æ„

```
vocab-analyzer/
â”œâ”€â”€ src/vocab_analyzer/          # æºä»£ç ï¼ˆ3,930è¡Œï¼‰
â”‚   â”œâ”€â”€ models/                  # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ word.py             # Wordæ•°æ®ç±»ï¼ˆ126è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ phrase.py           # Phraseæ•°æ®ç±»ï¼ˆ150è¡Œï¼‰
â”‚   â”‚   â””â”€â”€ analysis.py         # VocabularyAnalysisï¼ˆ267è¡Œï¼‰
â”‚   â”œâ”€â”€ extractors/              # æ–‡æœ¬æå–ï¼ˆ374è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ base.py             # BaseExtractoræŠ½è±¡ç±»
â”‚   â”‚   â”œâ”€â”€ txt_extractor.py    # TXTæ–‡ä»¶æ”¯æŒ
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py    # PDFæ–‡ä»¶æ”¯æŒ
â”‚   â”‚   â”œâ”€â”€ docx_extractor.py   # DOCXæ–‡ä»¶æ”¯æŒ
â”‚   â”‚   â””â”€â”€ json_extractor.py   # JSONæ–‡ä»¶æ”¯æŒ
â”‚   â”œâ”€â”€ processors/              # NLPå¤„ç†ï¼ˆ530è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ tokenizer.py        # åˆ†è¯å’Œè¯å½¢è¿˜åŸ
â”‚   â”‚   â””â”€â”€ phrase_detector.py  # åŠ¨è¯çŸ­è¯­æ£€æµ‹
â”‚   â”œâ”€â”€ matchers/                # ç­‰çº§åŒ¹é…ï¼ˆ416è¡Œï¼‰
â”‚   â”‚   â””â”€â”€ level_matcher.py    # CEFRåˆ†ç±»
â”‚   â”œâ”€â”€ analyzers/               # ç»Ÿè®¡åˆ†æï¼ˆ254è¡Œï¼‰
â”‚   â”‚   â””â”€â”€ statistics.py       # ç»Ÿè®¡åˆ†æ
â”‚   â”œâ”€â”€ exporters/               # è¾“å‡ºæ ¼å¼ï¼ˆ483è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ json_exporter.py    # JSONå¯¼å‡º
â”‚   â”‚   â”œâ”€â”€ csv_exporter.py     # CSVå¯¼å‡º
â”‚   â”‚   â””â”€â”€ markdown_exporter.py# Markdownå¯¼å‡º
â”‚   â”œâ”€â”€ core/                    # æ ¸å¿ƒé€»è¾‘ï¼ˆ549è¡Œï¼‰
â”‚   â”‚   â”œâ”€â”€ analyzer.py         # VocabularyAnalyzerå¤–è§‚
â”‚   â”‚   â””â”€â”€ config.py           # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ cli/                     # CLIç•Œé¢ï¼ˆ231è¡Œï¼‰
â”‚   â”‚   â””â”€â”€ main.py             # åŸºäºClickçš„CLI
â”‚   â””â”€â”€ utils/                   # å·¥å…·å‡½æ•°ï¼ˆ550è¡Œï¼‰
â”‚       â”œâ”€â”€ file_utils.py       # æ–‡ä»¶æ“ä½œ
â”‚       â”œâ”€â”€ text_utils.py       # æ–‡æœ¬å¤„ç†
â”‚       â””â”€â”€ cache.py            # ç¼“å­˜å·¥å…·
â”œâ”€â”€ data/                        # æ•°æ®èµ„æºï¼ˆ~205MBï¼‰
â”‚   â”œâ”€â”€ vocabularies/            # CEFRè¯æ±‡è¡¨
â”‚   â”‚   â””â”€â”€ cefr_wordlist.csv   # 43,699ä¸ªå·²åˆ†ç±»å•è¯
â”‚   â”œâ”€â”€ phrases/                 # åŠ¨è¯çŸ­è¯­
â”‚   â”‚   â””â”€â”€ phrasal_verbs.csv   # 124ä¸ªå¸¸ç”¨åŠ¨è¯
â”‚   â”œâ”€â”€ dictionaries/            # è¯å…¸
â”‚   â”‚   â””â”€â”€ ECDICT/             # 770,608è¯æ¡
â”‚   â”œâ”€â”€ sample_books/            # æ ·ä¾‹æ–‡æœ¬ï¼ˆ3æœ¬ä¹¦ï¼‰
â”‚   â””â”€â”€ mappings/                # CEFR-é›…æ€æ˜ å°„
â”œâ”€â”€ tests/                       # æµ‹è¯•å¥—ä»¶ï¼ˆ165è¡Œï¼‰
â”‚   â”œâ”€â”€ conftest.py             # æµ‹è¯•å›ºä»¶
â”‚   â”œâ”€â”€ unit/                   # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ integration/            # é›†æˆæµ‹è¯•
â”œâ”€â”€ scripts/                     # å·¥å…·è„šæœ¬ï¼ˆ400+è¡Œï¼‰
â”‚   â”œâ”€â”€ prepare_data.py         # æ•°æ®å‡†å¤‡
â”‚   â””â”€â”€ validate_data.py        # æ•°æ®éªŒè¯
â”œâ”€â”€ config/                      # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default_config.yaml     # é»˜è®¤è®¾ç½®
â”œâ”€â”€ docs/                        # æ–‡æ¡£ï¼ˆ107KB+ï¼‰
â”‚   â”œâ”€â”€ USER_GUIDE.md           # ç”¨æˆ·æŒ‡å—ï¼ˆ15KBï¼‰
â”‚   â”œâ”€â”€ EXAMPLES.md             # ç¤ºä¾‹ï¼ˆ16KBï¼‰
â”‚   â””â”€â”€ QUICK_REFERENCE.md      # å¿«é€Ÿå‚è€ƒï¼ˆ5.6KBï¼‰
â”œâ”€â”€ requirements.txt             # ä¾èµ–é¡¹
â”œâ”€â”€ setup.py                     # åŒ…å®‰è£…
â””â”€â”€ README.md                    # æœ¬æ–‡ä»¶
```

### ğŸ“Š æ€§èƒ½æŒ‡æ ‡

**åŸºå‡†æµ‹è¯•**ï¼ˆåœ¨MacBook Pro M1ä¸Šæµ‹è¯•ï¼‰ï¼š
- å°æ–‡ä»¶ï¼ˆ<5é¡µï¼‰ï¼šçº¦2ç§’
- ä¸­æ–‡ä»¶ï¼ˆ20-50é¡µï¼‰ï¼šçº¦15ç§’
- å¤§æ–‡ä»¶ï¼ˆ100+é¡µï¼‰ï¼š<60ç§’
- å†…å­˜ä½¿ç”¨ï¼š<400MBå³°å€¼

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- å…¨å±€spaCyæ¨¡å‹åŠ è½½ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
- å•è¯/è¯ç»„æŸ¥è¯¢LRUç¼“å­˜ï¼ˆ10,000æ¡ï¼‰
- æ‰¹é‡å¤„ç†ï¼ˆæ¯æ‰¹100å¥ï¼‰
- pandas DataFrameç´¢å¼•ä¼˜åŒ–

### ğŸ—‚ï¸ æ•°æ®æ¥æº

æœ¬é¡¹ç›®ä½¿ç”¨ä»¥ä¸‹å¼€æºæ•°æ®ï¼š

| èµ„æº | è§„æ¨¡ | è®¸å¯è¯ | æ¥æº |
|------|------|--------|------|
| **ECDICT** | 770,608è¯ | MIT | [skywind3000/ECDICT](https://github.com/skywind3000/ECDICT) |
| **Phrasal Verbs** | 124ä¸ªåŠ¨è¯ | å¼€æº | [Semigradsky/phrasal-verbs](https://github.com/Semigradsky/phrasal-verbs) |
| **æ ·ä¾‹ä¹¦ç±** | 3æœ¬ä¹¦ | å…¬æœ‰é¢†åŸŸ | [Project Gutenberg](https://www.gutenberg.org/) |

### ğŸ§ª å¼€å‘

#### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# å¸¦è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=vocab_analyzer --cov-report=html

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/unit/test_word.py -v

# è¯¦ç»†è¾“å‡º
pytest -vv
```

#### ä»£ç è´¨é‡

```bash
# æ ¼å¼åŒ–ä»£ç 
black src/ tests/
isort src/ tests/

# æ£€æŸ¥ä»£ç é£æ ¼
flake8 src/ tests/
pylint src/ --rcfile=.pylintrc

# ç±»å‹æ£€æŸ¥
mypy src/
```

#### Pre-commité’©å­

```bash
# å®‰è£…é’©å­ï¼ˆæ¯æ¬¡æäº¤æ—¶è¿è¡Œï¼‰
pre-commit install

# æ‰‹åŠ¨è¿è¡Œæ‰€æœ‰æ–‡ä»¶
pre-commit run --all-files
```

### âš™ï¸ é…ç½®

é»˜è®¤é…ç½®åœ¨`config/default_config.yaml`ã€‚ä½¿ç”¨è‡ªå®šä¹‰é…ç½®è¦†ç›–ï¼š

```bash
vocab-analyzer analyze book.txt --config my_config.yaml
```

**å…³é”®é…ç½®é€‰é¡¹**ï¼š
- NLPæ¨¡å‹å’Œæ‰¹å¤„ç†å¤§å°
- æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆè¯å…¸ã€è¯æ±‡è¡¨ï¼‰
- è¾“å‡ºæ ¼å¼å’Œæ¨¡æ¿
- åˆ†æå‚æ•°ï¼ˆæœ€å°é¢‘ç‡ã€ç­‰çº§é˜ˆå€¼ï¼‰
- æ—¥å¿—çº§åˆ«å’Œæ ¼å¼

### ğŸ—ºï¸ è·¯çº¿å›¾

#### âœ… ç¬¬ä¸€é˜¶æ®µï¼šMVPï¼ˆv0.1.0 - å·²å®Œæˆï¼‰
- [x] å¤šæ ¼å¼æ–‡æœ¬æå–ï¼ˆTXTã€PDFã€DOCXã€JSONï¼‰
- [x] CEFRç­‰çº§åˆ†ç±»ï¼ˆA1-C2+ï¼‰
- [x] åŠ¨è¯çŸ­è¯­æ£€æµ‹ï¼ˆ124ä¸ªåŠ¨è¯ï¼‰
- [x] ä¸­æ–‡ç¿»è¯‘ï¼ˆ770K+å•è¯ï¼‰
- [x] å¤šç§å¯¼å‡ºæ ¼å¼ï¼ˆJSONã€CSVã€Markdownï¼‰
- [x] å¸¦ä¸°å¯Œæ ¼å¼çš„CLI
- [x] å…¨é¢çš„ç»Ÿè®¡åˆ†æ

#### ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šåŠŸèƒ½å¢å¼ºï¼ˆè®¡åˆ’ä¸­ï¼‰
- [ ] æ‰©å±•åŠ¨è¯çŸ­è¯­è¯å…¸ï¼ˆ500+åŠ¨è¯ï¼‰
- [ ] é«˜çº§ä¾‹å¥æå–
- [ ] Ankiå¡ç»„å¯¼å‡ºæ ¼å¼
- [ ] Webç•Œé¢ï¼ˆFlask/FastAPIï¼‰
- [ ] æ‰¹é‡å¤„ç†æ¨¡å¼

#### ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šé«˜çº§åŠŸèƒ½ï¼ˆæœªæ¥ï¼‰
- [ ] è¿›åº¦è·Ÿè¸ªå’Œå­¦ä¹ åˆ†æ
- [ ] è‡ªå®šä¹‰è¯æ±‡è¡¨æ”¯æŒ
- [ ] éŸ³é¢‘å‘éŸ³é›†æˆ
- [ ] å¤šè¯­è¨€æ”¯æŒï¼ˆæ³•è¯­ã€å¾·è¯­ã€è¥¿ç­ç‰™è¯­ï¼‰
- [ ] ç§»åŠ¨åº”ç”¨

### ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹[CONTRIBUTING.md](CONTRIBUTING.md)äº†è§£æŒ‡å—ã€‚

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ï¼ˆ`git checkout -b feature/AmazingFeature`ï¼‰
3. æäº¤æ›´æ”¹ï¼ˆ`git commit -m 'Add some AmazingFeature'`ï¼‰
4. æ¨é€åˆ°åˆ†æ”¯ï¼ˆ`git push origin feature/AmazingFeature`ï¼‰
5. å¼€å¯Pull Request

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - è¯¦è§[LICENSE](LICENSE)æ–‡ä»¶ã€‚

### ğŸ™ è‡´è°¢

- **ECDICT** æä¾›å…¨é¢çš„è‹±æ±‰è¯å…¸
- **spaCy** æä¾›å¼ºå¤§çš„NLPå¤„ç†
- **Project Gutenberg** æä¾›å…¬æœ‰é¢†åŸŸä¹¦ç±
- æ‰€æœ‰å¼€æºè´¡çŒ®è€…

### ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ã€å»ºè®®æˆ–ç–‘é—®ï¼š
- ğŸ“– æŸ¥çœ‹[USER_GUIDE.md](docs/USER_GUIDE.md)äº†è§£è¯¦ç»†ç”¨æ³•
- ğŸ“ æŸ¥çœ‹[EXAMPLES.md](docs/EXAMPLES.md)äº†è§£å®è·µç¤ºä¾‹
- ğŸ› åœ¨GitHubä¸Šæäº¤issue
- ğŸ’¬ åœ¨GitHub Discussionsä¸­å‚ä¸è®¨è®º

---

**æ³¨æ„**ï¼šè¿™æ˜¯ä¸€ä¸ªä¸ªäººå­¦ä¹ å·¥å…·ï¼Œæ—¨åœ¨å¸®åŠ©è‹±è¯­å­¦ä¹ è€…åˆ†æä¹¦ç±ä¸­çš„è¯æ±‡ã€‚ä»…ä¾›æ•™è‚²ç›®çš„ä½¿ç”¨ã€‚

**å¼€å‘å›¢é˜Ÿ** | **MIT License** | **v0.1.0**
