# Implementation Plan: è‹±æ–‡ä¹¦è¯æ±‡ç­‰çº§åˆ†æå·¥å…·

**Branch**: `main` | **Date**: 2025-11-03 | **Spec**: [need.md](../need.md)
**Project**: English Book Vocabulary Level Analyzer
**Type**: CLI Application (Single Project)

---

## Summary

æ„å»ºä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ï¼Œå°†è‹±æ–‡ä¹¦ç±ï¼ˆTXT/PDF/DOCXï¼‰è½¬æ¢ä¸ºæŒ‰CEFRç­‰çº§ï¼ˆA1-C2ï¼‰åˆ†ç±»çš„å•è¯è¡¨ï¼ŒåŒ…å«ä¸­æ–‡é‡Šä¹‰å’Œä¾‹å¥ï¼Œå¸®åŠ©ç”¨æˆ·æœ‰é’ˆå¯¹æ€§åœ°èƒŒå•è¯ã€‚é‡‡ç”¨ç®¡é“æ¨¡å¼ï¼ˆPipeline Patternï¼‰å¤„ç†æ–‡æœ¬æµï¼Œå¤–è§‚æ¨¡å¼ï¼ˆFacade Patternï¼‰ç»Ÿä¸€åè°ƒå„æ¨¡å—ã€‚æ ¸å¿ƒæŠ€æœ¯æ ˆï¼šPython 3.10+ + spaCy + pandas + clickã€‚

---

## Technical Context

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

**Language/Version**: Python 3.10+
**Primary Dependencies**:
- spaCy 3.7 + en_core_web_sm æ¨¡å‹ï¼ˆNLPå¤„ç†ï¼‰
- PyPDF2 2.xï¼ˆPDFæ–‡æœ¬æå–ï¼‰
- python-docx 1.xï¼ˆWordæ–‡æ¡£æå–ï¼‰
- pandas 2.xï¼ˆæ•°æ®å¤„ç†å’Œç»Ÿè®¡ï¼‰
- click 8.xï¼ˆCLIæ¡†æ¶ï¼‰
- rich 13.xï¼ˆç»ˆç«¯ç¾åŒ–è¾“å‡ºï¼‰

**Development Tools**:
- pytest + pytest-covï¼ˆæµ‹è¯•å’Œè¦†ç›–ç‡ï¼‰
- black + isortï¼ˆä»£ç æ ¼å¼åŒ–ï¼‰
- pylint + flake8 + mypyï¼ˆä»£ç è´¨é‡æ£€æŸ¥ï¼‰
- pre-commitï¼ˆGit hooksï¼‰

**Storage**:
- CSVæ–‡ä»¶ï¼ˆè¯æ±‡è¡¨ã€è¯ç»„è¡¨ï¼‰
- JSONæ–‡ä»¶ï¼ˆé…ç½®ã€æ˜ å°„è¡¨ã€è¾“å‡ºç»“æœï¼‰
- æ— æ•°æ®åº“ä¾èµ–

**Testing**: pytest + unittest.mock
**Target Platform**: macOS/Linux/Windowsï¼ˆè·¨å¹³å°CLIï¼‰
**Project Type**: Single projectï¼ˆå•ä½“å‘½ä»¤è¡Œåº”ç”¨ï¼‰

### æ€§èƒ½ç›®æ ‡ä¸çº¦æŸ

**Performance Goals**:
- å°æ–‡ä»¶ï¼ˆ<5é¡µï¼‰ï¼š<5ç§’
- ä¸­æ–‡ä»¶ï¼ˆ20-50é¡µï¼‰ï¼š<30ç§’
- å¤§æ–‡ä»¶ï¼ˆ100é¡µï¼‰ï¼š<90ç§’
- å†…å­˜å³°å€¼ï¼š<500MB
- æ‹’ç»å¤„ç†ï¼š>50MBçš„æ–‡ä»¶

**Constraints**:
- å•çº¿ç¨‹å¤„ç†ï¼ˆä¸ä½¿ç”¨å¤šè¿›ç¨‹/å¤šçº¿ç¨‹ï¼‰
- spaCyæ¨¡å‹å…¨å±€åŠ è½½ä¸€æ¬¡
- è¯æ±‡æŸ¥è¯¢å¿…é¡»ä½¿ç”¨@lru_cacheç¼“å­˜
- spaCyæ‰¹å¤„ç†ï¼š100å¥/æ‰¹æ¬¡
- pandaså¿…é¡»ä½¿ç”¨ç´¢å¼•åŠ é€ŸæŸ¥è¯¢

**Scale/Scope**:
- æ”¯æŒè¯æ±‡é‡ï¼š5000-10000è¯
- è¯ç»„æ•°é‡ï¼š500+
- æ ·ä¾‹ä¹¦ç±ï¼š3-5æœ¬
- å•æ¬¡å¤„ç†ï¼š1æœ¬ä¹¦ï¼ˆä¸æ”¯æŒæ‰¹é‡ï¼‰

---

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### âœ… Simplicity Gates

- âœ… **Single Project**: CLIå·¥å…·ï¼Œä¸éœ€è¦å‰åç«¯åˆ†ç¦»
- âœ… **No Framework Overkill**: ä½¿ç”¨clickï¼ˆè½»é‡CLIæ¡†æ¶ï¼‰ï¼Œä¸éœ€è¦Flask/Django
- âœ… **Minimal Dependencies**: åªä½¿ç”¨å¿…éœ€çš„NLPå’Œæ–‡ä»¶å¤„ç†åº“
- âœ… **Direct Data Access**: ç›´æ¥è¯»å–CSV/JSONï¼Œä¸éœ€è¦ORMæˆ–æ•°æ®åº“
- âœ… **No Premature Abstraction**: å¤–è§‚æ¨¡å¼+ç®¡é“æ¨¡å¼ï¼Œæ¶æ„æ¸…æ™°ç®€å•

### âš ï¸ Complexity Justifications

| Potential Complexity | Why Needed | Simpler Alternative Rejected |
|---------------------|------------|------------------------------|
| spaCyä¾èµ–ï¼ˆ>100MBï¼‰ | è¯å½¢è¿˜åŸã€è¯æ€§æ ‡æ³¨å¿…éœ€ | æ­£åˆ™è¡¨è¾¾å¼æ— æ³•å‡†ç¡®å¤„ç†è¯å½¢å˜åŒ– |
| ç®¡é“æ¨¡å¼6ä¸ªæ¨¡å— | å„é˜¶æ®µèŒè´£æ˜ç¡®ï¼Œæ˜“æµ‹è¯• | å•ä¸ªå¤§å‡½æ•°éš¾ä»¥ç»´æŠ¤å’Œæ‰©å±• |
| dataclassæ•°æ®ç»“æ„ | ç±»å‹å®‰å…¨ï¼ŒIDEå‹å¥½ | å­—å…¸ä¼ é€’æ•°æ®æ˜“å‡ºé”™ï¼Œéš¾è°ƒè¯• |

---

## Architecture Overview

### è®¾è®¡æ¨¡å¼

#### 1. Pipeline Patternï¼ˆç®¡é“æ¨¡å¼ï¼‰

```
æ–‡ä»¶è¾“å…¥ â†’ æ–‡æœ¬æå– â†’ NLPå¤„ç† â†’ è¯ç»„è¯†åˆ« â†’
ç­‰çº§åŒ¹é… â†’ ç»Ÿè®¡åˆ†æ â†’ è¾“å‡ºç”Ÿæˆ â†’ ç»“æœæ–‡ä»¶
```

æ¯ä¸ªé˜¶æ®µç‹¬ç«‹ã€å¯æµ‹è¯•ã€å¯æ›¿æ¢ã€‚

#### 2. Facade Patternï¼ˆå¤–è§‚æ¨¡å¼ï¼‰

```python
class VocabularyAnalyzer:
    """ç»Ÿä¸€åè°ƒæ‰€æœ‰æ¨¡å—çš„å¤–è§‚ç±»"""
    def analyze(self, file_path: str) -> VocabularyAnalysis:
        # åè°ƒ6ä¸ªæ¨¡å—çš„æ‰§è¡Œ
        pass
```

CLIåªè°ƒç”¨`VocabularyAnalyzer`ï¼Œä¸ç›´æ¥è®¿é—®åº•å±‚æ¨¡å—ã€‚

### æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ˆdataclassï¼‰

```python
@dataclass
class Word:
    word: str              # è¯å½¢è¿˜åŸåçš„å•è¯
    level: str             # CEFRç­‰çº§ (A1-C2+)
    word_type: str         # è¯æ€§ (noun/verb/adjç­‰)
    definition_cn: str     # ä¸­æ–‡é‡Šä¹‰
    frequency: int         # å‡ºç°é¢‘æ¬¡
    examples: List[str]    # ä¾‹å¥

@dataclass
class Phrase:
    phrase: str            # è¯ç»„ (å¦‚ look up)
    type: str              # ç±»å‹ (phrasal_verb/collocation)
    level: str             # CEFRç­‰çº§
    separable: bool        # æ˜¯å¦å¯åˆ†ç¦»
    definition_cn: str     # ä¸­æ–‡é‡Šä¹‰
    frequency: int         # å‡ºç°é¢‘æ¬¡
    examples: List[str]    # ä¾‹å¥

@dataclass
class VocabularyAnalysis:
    metadata: Dict         # å…ƒæ•°æ®ï¼ˆæ–‡ä»¶åã€æ—¥æœŸç­‰ï¼‰
    statistics: Dict       # ç»Ÿè®¡æ•°æ®ï¼ˆå„ç­‰çº§è¯æ•°ï¼‰
    words_by_level: Dict[str, List[Word]]    # æŒ‰ç­‰çº§åˆ†ç±»çš„å•è¯
    phrases: List[Phrase]                     # è¯ç»„åˆ—è¡¨
    proper_nouns: List[Word]                  # ä¸“æœ‰åè¯
```

---

## Project Structure

### Documentation (this feature)

```text
.specify/
â”œâ”€â”€ implementation-plan.md           # æœ¬æ–‡ä»¶ï¼ˆæŠ€æœ¯å®ç°è®¡åˆ’ï¼‰
â”œâ”€â”€ story-0-data-preparation-spec.md # Story 0è§„æ ¼
â”œâ”€â”€ story-0-clarifications.md         # æ¾„æ¸…é—®é¢˜
â”œâ”€â”€ story-0-execution-summary.md      # Story 0æ‰§è¡Œæ€»ç»“
â””â”€â”€ templates/                         # Specifyæ¨¡æ¿
```

### Source Code (repository root)

```text
vocab-analyzer/                       # é¡¹ç›®æ ¹ç›®å½•
â”œâ”€â”€ README.md                          # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ requirements.txt                   # Pythonä¾èµ–
â”œâ”€â”€ setup.py                           # å®‰è£…é…ç½®
â”œâ”€â”€ pyproject.toml                     # é¡¹ç›®é…ç½®ï¼ˆblack/isort/mypyï¼‰
â”œâ”€â”€ .pre-commit-config.yaml            # Git hooksé…ç½®
â”‚
â”œâ”€â”€ config/                            # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ default_config.yaml            # é»˜è®¤é…ç½®ï¼ˆæ¨¡å‹ã€è·¯å¾„ã€æ‰¹æ¬¡å¤§å°ï¼‰
â”‚
â”œâ”€â”€ data/                              # æ•°æ®èµ„æºï¼ˆè§data/README.mdï¼‰
â”‚   â”œâ”€â”€ vocabularies/                  # CEFRè¯æ±‡è¡¨
â”‚   â”œâ”€â”€ phrases/                       # è¯ç»„è¯å…¸
â”‚   â”œâ”€â”€ dictionaries/                  # ä¸­è‹±è¯å…¸
â”‚   â”œâ”€â”€ sample_books/                  # æ ·ä¾‹ä¹¦ç±
â”‚   â””â”€â”€ mappings/                      # CEFR-IELTSæ˜ å°„è¡¨
â”‚
â”œâ”€â”€ src/                               # æºä»£ç 
â”‚   â”œâ”€â”€ vocab_analyzer/                # ä¸»åŒ…
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __main__.py                # å…¥å£ç‚¹ï¼ˆpython -m vocab_analyzerï¼‰
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/                    # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ word.py                # Word dataclass
â”‚   â”‚   â”‚   â”œâ”€â”€ phrase.py              # Phrase dataclass
â”‚   â”‚   â”‚   â””â”€â”€ analysis.py            # VocabularyAnalysis dataclass
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ extractors/                # æ–‡æœ¬æå–æ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py                # BaseExtractoræŠ½è±¡ç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ txt_extractor.py       # TXTæå–å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py       # PDFæå–å™¨
â”‚   â”‚   â”‚   â””â”€â”€ docx_extractor.py      # DOCXæå–å™¨
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ processors/                # NLPå¤„ç†æ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tokenizer.py           # åˆ†è¯å’Œè¯å½¢è¿˜åŸ
â”‚   â”‚   â”‚   â”œâ”€â”€ phrase_detector.py     # è¯ç»„è¯†åˆ«
â”‚   â”‚   â”‚   â””â”€â”€ proper_noun_filter.py  # ä¸“æœ‰åè¯è¯†åˆ«
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ matchers/                  # ç­‰çº§åŒ¹é…æ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ level_matcher.py       # CEFRç­‰çº§åŒ¹é…
â”‚   â”‚   â”‚   â””â”€â”€ dictionary_loader.py   # è¯å…¸åŠ è½½å™¨
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ analyzers/                 # ç»Ÿè®¡åˆ†ææ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ statistics.py          # ç»Ÿè®¡è®¡ç®—
â”‚   â”‚   â”‚   â””â”€â”€ example_extractor.py   # ä¾‹å¥æå–
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ exporters/                 # è¾“å‡ºç”Ÿæˆæ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ json_exporter.py       # JSONå¯¼å‡º
â”‚   â”‚   â”‚   â”œâ”€â”€ csv_exporter.py        # CSVå¯¼å‡º
â”‚   â”‚   â”‚   â””â”€â”€ markdown_exporter.py   # Markdownå¯¼å‡º
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒåè°ƒæ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ analyzer.py            # VocabularyAnalyzerï¼ˆå¤–è§‚ç±»ï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py            # Pipelineç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ config.py              # é…ç½®åŠ è½½
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ cli/                       # å‘½ä»¤è¡Œç•Œé¢
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py                # clickå‘½ä»¤å®šä¹‰
â”‚   â”‚   â”‚   â””â”€â”€ display.py             # richè¾“å‡ºæ ¼å¼åŒ–
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ file_utils.py          # æ–‡ä»¶æ“ä½œ
â”‚   â”‚       â”œâ”€â”€ text_utils.py          # æ–‡æœ¬å¤„ç†
â”‚   â”‚       â””â”€â”€ cache.py               # ç¼“å­˜è£…é¥°å™¨
â”‚
â”œâ”€â”€ tests/                             # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                    # pytesté…ç½®å’Œfixtures
â”‚   â”‚
â”‚   â”œâ”€â”€ unit/                          # å•å…ƒæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”‚   â”œâ”€â”€ test_processors.py
â”‚   â”‚   â”œâ”€â”€ test_matchers.py
â”‚   â”‚   â”œâ”€â”€ test_analyzers.py
â”‚   â”‚   â””â”€â”€ test_exporters.py
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/                   # é›†æˆæµ‹è¯•
â”‚   â”‚   â”œâ”€â”€ test_pipeline.py           # æµ‹è¯•å®Œæ•´ç®¡é“
â”‚   â”‚   â””â”€â”€ test_analyzer.py           # æµ‹è¯•VocabularyAnalyzer
â”‚   â”‚
â”‚   â””â”€â”€ fixtures/                      # æµ‹è¯•æ•°æ®
â”‚       â”œâ”€â”€ sample_text.txt
â”‚       â”œâ”€â”€ sample.pdf
â”‚       â””â”€â”€ expected_output.json
â”‚
â””â”€â”€ scripts/                           # è¾…åŠ©è„šæœ¬
    â”œâ”€â”€ prepare_data.py                # æ•°æ®å‡†å¤‡è„šæœ¬
    â”œâ”€â”€ download_model.py              # ä¸‹è½½spaCyæ¨¡å‹
    â””â”€â”€ validate_data.py               # éªŒè¯æ•°æ®å®Œæ•´æ€§
```

**Structure Decision**:
é€‰æ‹©å•é¡¹ç›®ç»“æ„ï¼ˆOption 1ï¼‰ï¼Œå› ä¸ºï¼š
1. CLIå·¥å…·æ— éœ€å‰åç«¯åˆ†ç¦»
2. æ¨¡å—æŒ‰åŠŸèƒ½å‚ç›´åˆ‡åˆ†ï¼ˆextractors/processors/matchersç­‰ï¼‰
3. æ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
4. ä½¿ç”¨`core/`ç›®å½•ç»Ÿä¸€åè°ƒå„æ¨¡å—ï¼ˆå¤–è§‚æ¨¡å¼ï¼‰

---

## Module Responsibilities

### 1. Extractorsï¼ˆæ–‡æœ¬æå–æ¨¡å—ï¼‰

**èŒè´£**: ä»ä¸åŒæ ¼å¼çš„æ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬

**æ¥å£**:
```python
class BaseExtractor(ABC):
    @abstractmethod
    def extract(self, file_path: str) -> str:
        """æå–æ–‡æœ¬ï¼Œè¿”å›çº¯æ–‡æœ¬å­—ç¬¦ä¸²"""
        pass
```

**å®ç°**:
- `TxtExtractor`: UTF-8è§£ç 
- `PdfExtractor`: PyPDF2æå–æ–‡å­—
- `DocxExtractor`: python-docxæå–æ®µè½

**è¾“å‡º**: çº¯æ–‡æœ¬å­—ç¬¦ä¸²

---

### 2. Processorsï¼ˆNLPå¤„ç†æ¨¡å—ï¼‰

**èŒè´£**: åˆ†è¯ã€è¯å½¢è¿˜åŸã€è¯ç»„è¯†åˆ«ã€ä¸“æœ‰åè¯è¿‡æ»¤

**æ¥å£**:
```python
class Tokenizer:
    def process(self, text: str) -> List[Token]:
        """åˆ†è¯å’Œè¯å½¢è¿˜åŸ"""
        pass

class PhraseDetector:
    def detect(self, tokens: List[Token]) -> List[Phrase]:
        """è¯†åˆ«è¯ç»„ï¼ˆåŒ…æ‹¬åˆ†ç¦»çš„åŠ¨è¯çŸ­è¯­ï¼‰"""
        pass
```

**å…³é”®æŠ€æœ¯**:
- spaCyçš„`nlp.pipe()`æ‰¹å¤„ç†ï¼ˆ100å¥/æ‰¹ï¼‰
- ä¾å­˜å¥æ³•åˆ†æè¯†åˆ«åˆ†ç¦»è¯ç»„
- è¯æ€§æ ‡æ³¨ï¼ˆPROPNï¼‰è¯†åˆ«ä¸“æœ‰åè¯

**è¾“å‡º**: Tokenåˆ—è¡¨ + Phraseåˆ—è¡¨

---

### 3. Matchersï¼ˆç­‰çº§åŒ¹é…æ¨¡å—ï¼‰

**èŒè´£**: å°†å•è¯å’Œè¯ç»„åŒ¹é…åˆ°CEFRç­‰çº§ï¼Œæ·»åŠ ä¸­æ–‡é‡Šä¹‰

**æ¥å£**:
```python
class LevelMatcher:
    @lru_cache(maxsize=10000)
    def match_word(self, word: str) -> Optional[WordInfo]:
        """åŒ¹é…å•è¯ç­‰çº§å’Œé‡Šä¹‰ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        pass
```

**å…³é”®æŠ€æœ¯**:
- pandas DataFrame + ç´¢å¼•åŠ é€ŸæŸ¥è¯¢
- @lru_cacheç¼“å­˜æŸ¥è¯¢ç»“æœ
- è¶…çº²è¯æ ‡è®°ä¸ºC2+

**è¾“å‡º**: å¸¦ç­‰çº§å’Œé‡Šä¹‰çš„Wordå¯¹è±¡åˆ—è¡¨

---

### 4. Analyzersï¼ˆç»Ÿè®¡åˆ†ææ¨¡å—ï¼‰

**èŒè´£**: ç»Ÿè®¡è¯æ±‡åˆ†å¸ƒã€æå–ä¾‹å¥

**æ¥å£**:
```python
class StatisticsAnalyzer:
    def analyze(self, words: List[Word]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ•°æ®"""
        pass

class ExampleExtractor:
    def extract(self, word: str, text: str, max_examples: int = 3) -> List[str]:
        """æå–ä¾‹å¥"""
        pass
```

**è¾“å‡º**: ç»Ÿè®¡æ•°æ®å­—å…¸ + ä¾‹å¥åˆ—è¡¨

---

### 5. Exportersï¼ˆè¾“å‡ºç”Ÿæˆæ¨¡å—ï¼‰

**èŒè´£**: ç”ŸæˆJSON/CSV/Markdownæ ¼å¼çš„è¾“å‡ºæ–‡ä»¶

**æ¥å£**:
```python
class BaseExporter(ABC):
    @abstractmethod
    def export(self, analysis: VocabularyAnalysis, output_path: str) -> None:
        """å¯¼å‡ºåˆ†æç»“æœ"""
        pass
```

**è¾“å‡º**: æ–‡ä»¶ï¼ˆJSON/CSV/MDï¼‰

---

### 6. Coreï¼ˆæ ¸å¿ƒåè°ƒæ¨¡å—ï¼‰

**èŒè´£**: åè°ƒæ•´ä¸ªåˆ†ææµç¨‹ï¼ˆå¤–è§‚æ¨¡å¼ï¼‰

**æ¥å£**:
```python
class VocabularyAnalyzer:
    def __init__(self, config: Config):
        """åˆå§‹åŒ–å¹¶åŠ è½½spaCyæ¨¡å‹ã€è¯æ±‡è¡¨"""
        pass

    def analyze(self, file_path: str) -> VocabularyAnalysis:
        """æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        # 1. æå–æ–‡æœ¬
        # 2. NLPå¤„ç†
        # 3. ç­‰çº§åŒ¹é…
        # 4. ç»Ÿè®¡åˆ†æ
        # 5. è¿”å›ç»“æœ
        pass
```

**è¾“å‡º**: VocabularyAnalysiså¯¹è±¡

---

## Data Flow

```
ç”¨æˆ·è¾“å…¥æ–‡ä»¶è·¯å¾„
    â†“
CLI (main.py)
    â†“
VocabularyAnalyzer.analyze()
    â†“
1. Extractor.extract() â†’ çº¯æ–‡æœ¬
    â†“
2. Tokenizer.process() â†’ List[Token]
    â†“
3. PhraseDetector.detect() â†’ List[Phrase]
    â†“
4. LevelMatcher.match_word() â†’ List[Word]
    â†“
5. StatisticsAnalyzer.analyze() â†’ Dict
    â†“
6. ExampleExtractor.extract() â†’ æ·»åŠ ä¾‹å¥
    â†“
7. ç»„è£… VocabularyAnalysis å¯¹è±¡
    â†“
Exporter.export() â†’ è¾“å‡ºæ–‡ä»¶
    â†“
CLIæ˜¾ç¤ºç»Ÿè®¡ç»“æœï¼ˆrichæ ¼å¼åŒ–ï¼‰
```

---

## Configuration Management

### default_config.yaml

```yaml
# NLPæ¨¡å‹é…ç½®
nlp:
  model: "en_core_web_sm"
  batch_size: 100
  disable_components: ["ner"]  # ç¦ç”¨ä¸éœ€è¦çš„ç»„ä»¶

# æ–‡ä»¶å¤„ç†é…ç½®
files:
  max_size_mb: 50
  encoding: "utf-8"

# æ•°æ®è·¯å¾„é…ç½®
data:
  vocabularies_dir: "data/vocabularies"
  phrases_file: "data/phrases/phrasal_verbs.csv"
  dictionary_file: "data/dictionaries/ecdict_core.csv"
  mappings_file: "data/mappings/cefr_ielts_mapping.json"

# æ€§èƒ½é…ç½®
performance:
  cache_size: 10000
  max_examples: 3

# è¾“å‡ºé…ç½®
output:
  default_format: "json"
  include_examples: true
  include_statistics: true
```

---

## CLI Interface Design

### å‘½ä»¤è¡Œç”¨æ³•

```bash
# åŸºæœ¬ç”¨æ³•
vocab-analyzer input.txt

# æŒ‡å®šè¾“å‡ºæ ¼å¼
vocab-analyzer input.pdf --format json,csv,md

# æŒ‡å®šè¾“å‡ºç›®å½•
vocab-analyzer input.docx --output ./results

# æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
vocab-analyzer input.txt --verbose

# åªè¾“å‡ºç‰¹å®šç­‰çº§
vocab-analyzer input.txt --levels B2,C1,C2

# æ˜¾ç¤ºå¸®åŠ©
vocab-analyzer --help
```

### è¾“å‡ºç¤ºä¾‹ï¼ˆrichæ ¼å¼åŒ–ï¼‰

```
ğŸ“š åˆ†ææ–‡ä»¶: pride_and_prejudice.txt
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

[1/6] æå–æ–‡æœ¬... âœ“ (2.3s)
[2/6] åˆ†è¯ä¸è¯å½¢è¿˜åŸ... âœ“ (8.7s)
[3/6] è¯†åˆ«è¯ç»„... âœ“ (3.2s)
[4/6] åŒ¹é…è¯æ±‡ç­‰çº§... âœ“ (1.8s)
[5/6] è·å–ä¸­æ–‡é‡Šä¹‰... âœ“ (12.4s)
[6/6] ç”Ÿæˆè¾“å‡ºæ–‡ä»¶... âœ“ (0.9s)

ğŸ“Š ç»Ÿè®¡ç»“æœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ€»å•è¯æ•°: 12,450
ç‹¬ç«‹å•è¯: 3,876
è¯†åˆ«è¯ç»„: 234

ç­‰çº§åˆ†å¸ƒ:
  A2 (KET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  850 è¯ (21.9%)
  B1 (PET)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1020 è¯ (26.3%)
  B2 (FCE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  680 è¯ (17.5%)
  C1 (CAE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  450 è¯ (11.6%)
  C2 (CPE)  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  320 è¯ (8.3%)
  C2+ (è¶…çº²) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  556 è¯ (14.4%)

âœ… è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆ:
  â€¢ pride_and_prejudice_vocabulary.json
  â€¢ pride_and_prejudice_vocabulary.csv
  â€¢ pride_and_prejudice_vocabulary.md

ğŸ’¡ å»ºè®®: è¯¥ä¹¦ä¸»è¦ä½¿ç”¨ B1-B2 è¯æ±‡ï¼Œé€‚åˆé›…æ€ 5.5-6.5 åˆ†æ°´å¹³é˜…è¯»
```

---

## Performance Optimization Strategies

### 1. spaCyä¼˜åŒ–

```python
# å…¨å±€åŠ è½½æ¨¡å‹ï¼ˆä»…åŠ è½½ä¸€æ¬¡ï¼‰
class VocabularyAnalyzer:
    _nlp = None

    @classmethod
    def get_nlp(cls):
        if cls._nlp is None:
            cls._nlp = spacy.load("en_core_web_sm", disable=["ner"])
        return cls._nlp

# æ‰¹å¤„ç†æ–‡æœ¬
def process_in_batches(texts: List[str], batch_size: int = 100):
    nlp = VocabularyAnalyzer.get_nlp()
    for doc in nlp.pipe(texts, batch_size=batch_size):
        yield doc
```

### 2. è¯æ±‡æŸ¥è¯¢ç¼“å­˜

```python
from functools import lru_cache

class LevelMatcher:
    @lru_cache(maxsize=10000)
    def match_word(self, word: str) -> Optional[WordInfo]:
        # æŸ¥è¯¢è¢«ç¼“å­˜ï¼Œé‡å¤è¯æ±‡ç›´æ¥è¿”å›
        return self._lookup_in_dataframe(word)
```

### 3. pandasç´¢å¼•åŠ é€Ÿ

```python
# åŠ è½½è¯æ±‡è¡¨æ—¶åˆ›å»ºç´¢å¼•
df = pd.read_csv("vocabulary.csv")
df.set_index("word", inplace=True)  # ä½¿ç”¨wordåˆ—ä½œä¸ºç´¢å¼•

# æŸ¥è¯¢æ—¶ä½¿ç”¨.locï¼ˆO(1)å¤æ‚åº¦ï¼‰
word_info = df.loc[word]
```

### 4. å†…å­˜ç®¡ç†

```python
# åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡
def analyze(self, file_path: str):
    text = self.extract_text(file_path)
    tokens = self.process_text(text)
    del text  # é‡Šæ”¾åŸå§‹æ–‡æœ¬å†…å­˜
    # ...ç»§ç»­å¤„ç†
```

---

## Error Handling Strategy

### æ–‡ä»¶å¤„ç†é”™è¯¯

```python
class FileError(Exception):
    """æ–‡ä»¶ç›¸å…³é”™è¯¯åŸºç±»"""
    pass

class FileTooLargeError(FileError):
    """æ–‡ä»¶è¶…è¿‡50MB"""
    pass

class UnsupportedFormatError(FileError):
    """ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
    pass
```

### ç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤º

```python
try:
    analyzer.analyze(file_path)
except FileTooLargeError:
    console.print("[red]âŒ æ–‡ä»¶è¿‡å¤§ï¼ˆ>50MBï¼‰ï¼Œè¯·ä½¿ç”¨è¾ƒå°çš„æ–‡ä»¶[/red]")
except UnsupportedFormatError:
    console.print("[red]âŒ ä¸æ”¯æŒçš„æ ¼å¼ï¼Œè¯·ä½¿ç”¨ .txt, .pdf æˆ– .docx æ–‡ä»¶[/red]")
except Exception as e:
    console.print(f"[red]âŒ å¤„ç†å¤±è´¥: {str(e)}[/red]")
```

---

## Testing Strategy

### å•å…ƒæµ‹è¯•ï¼ˆUnit Testsï¼‰

æ¯ä¸ªæ¨¡å—ç‹¬ç«‹æµ‹è¯•ï¼Œä½¿ç”¨mockéš”ç¦»ä¾èµ–ï¼š

```python
# tests/unit/test_tokenizer.py
def test_tokenizer_lemmatization():
    tokenizer = Tokenizer()
    tokens = tokenizer.process("I went to school yesterday")
    assert tokens[1].lemma == "go"  # went â†’ go
```

### é›†æˆæµ‹è¯•ï¼ˆIntegration Testsï¼‰

æµ‹è¯•å®Œæ•´æµç¨‹ï¼š

```python
# tests/integration/test_pipeline.py
def test_full_pipeline():
    analyzer = VocabularyAnalyzer(config)
    result = analyzer.analyze("tests/fixtures/sample.txt")
    assert result.statistics["B1"] > 0
    assert len(result.words_by_level["B1"]) > 0
```

### æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

- å•å…ƒæµ‹è¯•è¦†ç›–ç‡ï¼š>80%
- é›†æˆæµ‹è¯•ï¼šè¦†ç›–æ ¸å¿ƒæµç¨‹
- å…³é”®æ¨¡å—ï¼ˆTokenizer, LevelMatcherï¼‰ï¼š>90%

---

## Development Phases

### Phase 0: ç¯å¢ƒå‡†å¤‡ï¼ˆ1å¤©ï¼‰
- [ ] åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„
- [ ] é…ç½®pyproject.tomlã€requirements.txt
- [ ] é…ç½®pre-commit hooks
- [ ] ä¸‹è½½spaCyæ¨¡å‹ï¼š`python -m spacy download en_core_web_sm`
- [ ] å®ŒæˆStory 0æ•°æ®å‡†å¤‡

### Phase 1: æ ¸å¿ƒæ¨¡å—å¼€å‘ï¼ˆStory 1, 1å‘¨ï¼‰
- [ ] å®ç°Extractorsï¼ˆTXT/PDF/DOCXï¼‰
- [ ] å®ç°Tokenizerï¼ˆåˆ†è¯+è¯å½¢è¿˜åŸï¼‰
- [ ] å®ç°LevelMatcherï¼ˆç­‰çº§åŒ¹é…ï¼‰
- [ ] å®ç°åŸºç¡€VocabularyAnalyzer
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡>70%

### Phase 2: è¾“å‡ºå’ŒCLIï¼ˆStory 2, 3-4å¤©ï¼‰
- [ ] å®ç°StatisticsAnalyzer
- [ ] å®ç°Exportersï¼ˆJSON/CSV/MDï¼‰
- [ ] å®ç°CLIå‘½ä»¤ï¼ˆclickï¼‰
- [ ] å®ç°richæ ¼å¼åŒ–è¾“å‡º
- [ ] é›†æˆæµ‹è¯•é€šè¿‡

### Phase 3: é«˜çº§åŠŸèƒ½ï¼ˆStory 3-4, 1-2å‘¨ï¼‰
- [ ] å®ç°PhraseDetectorï¼ˆè¯ç»„è¯†åˆ«ï¼‰
- [ ] å®ç°ExampleExtractorï¼ˆä¾‹å¥æå–ï¼‰
- [ ] é›†æˆä¸­æ–‡é‡Šä¹‰
- [ ] æ€§èƒ½ä¼˜åŒ–

### Phase 4: å®Œå–„å’Œæµ‹è¯•ï¼ˆStory 5, 1å‘¨ï¼‰
- [ ] æ·»åŠ ä¾‹å¥æå–
- [ ] å®Œå–„CLIè¾“å‡º
- [ ] æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
- [ ] æ–‡æ¡£ç¼–å†™

---

## Dependencies Management

### requirements.txt

```txt
# Core NLP
spacy>=3.7.0,<4.0.0
# è¿è¡Œåæ‰§è¡Œ: python -m spacy download en_core_web_sm

# File processing
PyPDF2>=2.0.0,<3.0.0
python-docx>=1.0.0,<2.0.0

# Data processing
pandas>=2.0.0,<3.0.0

# CLI
click>=8.1.0,<9.0.0
rich>=13.0.0,<14.0.0

# Configuration
PyYAML>=6.0,<7.0

# Development
pytest>=7.4.0
pytest-cov>=4.1.0
black>=23.0.0
isort>=5.12.0
pylint>=2.17.0
flake8>=6.0.0
mypy>=1.4.0
pre-commit>=3.3.0
```

---

## Risk Mitigation

### æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| spaCyæ¨¡å‹åŠ è½½æ…¢ | å¯åŠ¨æ—¶é—´é•¿ | é«˜ | å…¨å±€åŠ è½½ä¸€æ¬¡ï¼Œæç¤ºç”¨æˆ·ç­‰å¾… |
| è¯ç»„è¯†åˆ«ä¸å‡†ç¡® | åŠŸèƒ½è´¨é‡ä¸‹é™ | ä¸­ | Phase 1å…ˆä¸åšï¼ŒPhase 3å†ä¼˜åŒ– |
| å¤§æ–‡ä»¶å†…å­˜æº¢å‡º | ç¨‹åºå´©æºƒ | ä¸­ | é™åˆ¶50MBï¼Œæ‰¹å¤„ç†æ–‡æœ¬ |
| PDFæå–å¤±è´¥ | éƒ¨åˆ†æ–‡ä»¶æ— æ³•å¤„ç† | ä½ | æ•è·å¼‚å¸¸ï¼Œæç¤ºç”¨æˆ·è½¬æ¢æ ¼å¼ |

---

## Success Criteria

### åŠŸèƒ½å®Œæ•´æ€§
- âœ… æ”¯æŒTXT/PDF/DOCXä¸‰ç§æ ¼å¼
- âœ… è¯å½¢è¿˜åŸå‡†ç¡®ç‡>95%
- âœ… ç­‰çº§åŒ¹é…å‡†ç¡®ç‡>95%ï¼ˆåŸºäºè¯æ±‡è¡¨ï¼‰
- âœ… è¾“å‡ºJSON/CSV/MDä¸‰ç§æ ¼å¼

### æ€§èƒ½è¾¾æ ‡
- âœ… 100é¡µä¹¦ç±<60ç§’
- âœ… å†…å­˜å³°å€¼<500MB
- âœ… æµ‹è¯•è¦†ç›–ç‡>80%

### ç”¨æˆ·ä½“éªŒ
- âœ… CLIè¾“å‡ºæ¸…æ™°ç¾è§‚ï¼ˆrichæ ¼å¼åŒ–ï¼‰
- âœ… é”™è¯¯æç¤ºå‹å¥½
- âœ… è¿›åº¦å®æ—¶æ˜¾ç¤º

---

## Next Actions

1. âœ… **å®ŒæˆStory 0æ•°æ®å‡†å¤‡**ï¼ˆå·²è°ƒç ”ï¼Œå¾…ä¸‹è½½ï¼‰
2. â³ **åˆ›å»ºé¡¹ç›®ç»“æ„**ï¼ˆæŒ‰æœ¬planæ‰§è¡Œï¼‰
3. â³ **é…ç½®å¼€å‘ç¯å¢ƒ**ï¼ˆrequirements.txt + pre-commitï¼‰
4. â³ **å¼€å§‹Story 1å¼€å‘**ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰

---

**Plan Version**: 1.0
**Created**: 2025-11-03
**Status**: âœ… è§„åˆ’å®Œæˆï¼Œå‡†å¤‡æ‰§è¡Œ
**Estimated Total Time**: 4-5å‘¨
